{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense                           \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import patsy\n",
    "from patsy import dmatrices\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input                           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now we upload the files and we see what come out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['y', 'g_pop_NUTS2', 'lag1ratio.tot_pop_NUTS2', 'lag1_under15',\n",
       "       'lag1_15_24', 'lag1_25_44', 'lag1_45_64', 'lag1_65', 'lag2_15_24',\n",
       "       'lag2_25_44', 'lag1_g_under15', 'lag1_g_15_24', 'lag1_g_25_44',\n",
       "       'lag1_g_45_64', 'lag1_g_65', 'log_quot', 'lag1_quot_under15',\n",
       "       'lag1_quot_15_24', 'lag1_quot_25_44', 'lag1_quot_45_64', 'lag1_quot_65',\n",
       "       'lag2_quot_15_24', 'ctry_prop_under15', 'ctry_prop_15_24',\n",
       "       'ctry_prop_25_44', 'ctry_prop_45_64', 'ctry_prop_65',\n",
       "       'lag1_ctry_prop_15_24', 'lag1_ctry_prop_25_44', 'lag1_ctry_prop_45_64',\n",
       "       'lag2_ctry_prop_15_24', 'lag2_ctry_prop_25_44',\n",
       "       'lag1_ctry_pop_g_under15', 'lag1_ctry_pop_g_15_24',\n",
       "       'lag1_ctry_pop_g_45_64', 'lag1_ctry_pop_g_65', 'cluster_1', 'cluster_2',\n",
       "       'cluster_3', 'urb_frac'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train_data.csv', index_col=[0])\n",
    "test_data = pd.read_csv('data/test_data.csv', index_col=[0])\n",
    "train_data.rename(columns={'g.pop.NUTS2': 'g_pop_NUTS2', 'log.quot':'log_quot'}, inplace=True)\n",
    "test_data.rename(columns={'g.pop.NUTS2': 'g_pop_NUTS2',  'log.quot':'log_quot'}, inplace=True)\n",
    "\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define formula and design matrices\n",
    "formula = \"y ~ g_pop_NUTS2 + lag1_under15 + lag1_25_44 + lag1_45_64 + lag1_65 + lag2_15_24 + lag2_25_44 + lag1_g_15_24 + lag1_g_25_44 + lag1_g_45_64 + lag1_g_65 + log_quot + lag1_quot_15_24 + lag1_quot_25_44 + lag1_quot_45_64 + lag1_quot_65 + lag2_quot_15_24 + ctry_prop_under15 + ctry_prop_15_24 + ctry_prop_45_64 + ctry_prop_65 + lag2_ctry_prop_15_24 + lag1_ctry_pop_g_under15 + lag1_ctry_pop_g_45_64 + lag1_ctry_pop_g_65 + cluster_1 + cluster_2 + cluster_3 + urb_frac\"\n",
    "\n",
    "# Generate design matrices\n",
    "y, COV = dmatrices(formula, train_data)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "COV.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Build a TensorFlow model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(COV.shape[1],)),\n",
    "    layers.Dense(27 , activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.fit(COV, y, epochs=10, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(COV)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y',\n",
       " 'g_pop_NUTS2',\n",
       " 'lag1_under15',\n",
       " 'lag1_25_44',\n",
       " 'lag1_45_64',\n",
       " 'lag1_65',\n",
       " 'lag2_15_24',\n",
       " 'lag2_25_44',\n",
       " 'lag1_g_15_24',\n",
       " 'lag1_g_25_44',\n",
       " 'lag1_g_45_64',\n",
       " 'lag1_g_65',\n",
       " 'log_quot',\n",
       " 'lag1_quot_15_24',\n",
       " 'lag1_quot_25_44',\n",
       " 'lag1_quot_45_64',\n",
       " 'lag1_quot_65',\n",
       " 'lag2_quot_15_24',\n",
       " 'ctry_prop_under15',\n",
       " 'ctry_prop_15_24',\n",
       " 'ctry_prop_45_64',\n",
       " 'ctry_prop_65',\n",
       " 'lag2_ctry_prop_15_24',\n",
       " 'lag1_ctry_pop_g_under15',\n",
       " 'lag1_ctry_pop_g_45_64',\n",
       " 'lag1_ctry_pop_g_65',\n",
       " 'cluster_1',\n",
       " 'cluster_2',\n",
       " 'cluster_3',\n",
       " 'urb_frac']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "selected_columns = ['y'] +COV.design_info.column_names[1:]\n",
    "\n",
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SIEVE_funcs\n",
    "from SIEVE_funcs import fit_mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 996 entries, 1 to 996\n",
      "Data columns (total 30 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   y                        996 non-null    float64\n",
      " 1   g_pop_NUTS2              996 non-null    float64\n",
      " 2   lag1_under15             996 non-null    float64\n",
      " 3   lag1_25_44               996 non-null    float64\n",
      " 4   lag1_45_64               996 non-null    float64\n",
      " 5   lag1_65                  996 non-null    float64\n",
      " 6   lag2_15_24               996 non-null    float64\n",
      " 7   lag2_25_44               996 non-null    float64\n",
      " 8   lag1_g_15_24             996 non-null    float64\n",
      " 9   lag1_g_25_44             996 non-null    float64\n",
      " 10  lag1_g_45_64             996 non-null    float64\n",
      " 11  lag1_g_65                996 non-null    float64\n",
      " 12  log_quot                 996 non-null    float64\n",
      " 13  lag1_quot_15_24          996 non-null    float64\n",
      " 14  lag1_quot_25_44          996 non-null    float64\n",
      " 15  lag1_quot_45_64          996 non-null    float64\n",
      " 16  lag1_quot_65             996 non-null    float64\n",
      " 17  lag2_quot_15_24          996 non-null    float64\n",
      " 18  ctry_prop_under15        996 non-null    float64\n",
      " 19  ctry_prop_15_24          996 non-null    float64\n",
      " 20  ctry_prop_45_64          996 non-null    float64\n",
      " 21  ctry_prop_65             996 non-null    float64\n",
      " 22  lag2_ctry_prop_15_24     996 non-null    float64\n",
      " 23  lag1_ctry_pop_g_under15  996 non-null    float64\n",
      " 24  lag1_ctry_pop_g_45_64    996 non-null    float64\n",
      " 25  lag1_ctry_pop_g_65       996 non-null    float64\n",
      " 26  cluster_1                996 non-null    int64  \n",
      " 27  cluster_2                996 non-null    int64  \n",
      " 28  cluster_3                996 non-null    int64  \n",
      " 29  urb_frac                 996 non-null    float64\n",
      "dtypes: float64(27), int64(3)\n",
      "memory usage: 241.2 KB\n"
     ]
    }
   ],
   "source": [
    "training_data = train_data[selected_columns]\n",
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>g_pop_NUTS2</th>\n",
       "      <th>lag1_under15</th>\n",
       "      <th>lag1_25_44</th>\n",
       "      <th>lag1_45_64</th>\n",
       "      <th>lag1_65</th>\n",
       "      <th>lag2_15_24</th>\n",
       "      <th>lag2_25_44</th>\n",
       "      <th>lag1_g_15_24</th>\n",
       "      <th>lag1_g_25_44</th>\n",
       "      <th>...</th>\n",
       "      <th>ctry_prop_45_64</th>\n",
       "      <th>ctry_prop_65</th>\n",
       "      <th>lag2_ctry_prop_15_24</th>\n",
       "      <th>lag1_ctry_pop_g_under15</th>\n",
       "      <th>lag1_ctry_pop_g_45_64</th>\n",
       "      <th>lag1_ctry_pop_g_65</th>\n",
       "      <th>cluster_1</th>\n",
       "      <th>cluster_2</th>\n",
       "      <th>cluster_3</th>\n",
       "      <th>urb_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.174051</td>\n",
       "      <td>-0.548777</td>\n",
       "      <td>-0.093245</td>\n",
       "      <td>0.898419</td>\n",
       "      <td>-0.966361</td>\n",
       "      <td>0.099501</td>\n",
       "      <td>0.376766</td>\n",
       "      <td>0.244323</td>\n",
       "      <td>-0.831105</td>\n",
       "      <td>1.281140</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.015873</td>\n",
       "      <td>-0.942237</td>\n",
       "      <td>0.614698</td>\n",
       "      <td>0.922978</td>\n",
       "      <td>-0.788814</td>\n",
       "      <td>-0.861139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.279664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.045550</td>\n",
       "      <td>-0.040484</td>\n",
       "      <td>0.471993</td>\n",
       "      <td>0.883495</td>\n",
       "      <td>-0.742478</td>\n",
       "      <td>-0.353670</td>\n",
       "      <td>0.387420</td>\n",
       "      <td>0.148629</td>\n",
       "      <td>-1.324951</td>\n",
       "      <td>1.413474</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.015873</td>\n",
       "      <td>-0.942237</td>\n",
       "      <td>0.614698</td>\n",
       "      <td>0.922978</td>\n",
       "      <td>-0.788814</td>\n",
       "      <td>-0.861139</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.390826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.019228</td>\n",
       "      <td>0.218589</td>\n",
       "      <td>-0.776667</td>\n",
       "      <td>1.659671</td>\n",
       "      <td>-0.242451</td>\n",
       "      <td>0.083935</td>\n",
       "      <td>-0.334807</td>\n",
       "      <td>0.948677</td>\n",
       "      <td>-1.452726</td>\n",
       "      <td>1.508788</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.015873</td>\n",
       "      <td>-0.942237</td>\n",
       "      <td>0.614698</td>\n",
       "      <td>0.922978</td>\n",
       "      <td>-0.788814</td>\n",
       "      <td>-0.861139</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.277729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.059381</td>\n",
       "      <td>-0.545698</td>\n",
       "      <td>0.518554</td>\n",
       "      <td>1.131020</td>\n",
       "      <td>-1.306223</td>\n",
       "      <td>-0.382344</td>\n",
       "      <td>0.702631</td>\n",
       "      <td>0.551440</td>\n",
       "      <td>-1.219143</td>\n",
       "      <td>1.203153</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.015873</td>\n",
       "      <td>-0.942237</td>\n",
       "      <td>0.614698</td>\n",
       "      <td>0.922978</td>\n",
       "      <td>-0.788814</td>\n",
       "      <td>-0.861139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.557908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.071665</td>\n",
       "      <td>-0.389150</td>\n",
       "      <td>0.245252</td>\n",
       "      <td>1.114368</td>\n",
       "      <td>-1.186103</td>\n",
       "      <td>-0.239795</td>\n",
       "      <td>0.709157</td>\n",
       "      <td>0.366296</td>\n",
       "      <td>-1.215400</td>\n",
       "      <td>1.476652</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.015873</td>\n",
       "      <td>-0.942237</td>\n",
       "      <td>0.614698</td>\n",
       "      <td>0.922978</td>\n",
       "      <td>-0.788814</td>\n",
       "      <td>-0.861139</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.467357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>1.346680</td>\n",
       "      <td>0.772758</td>\n",
       "      <td>-0.370570</td>\n",
       "      <td>-2.604024</td>\n",
       "      <td>1.429803</td>\n",
       "      <td>1.807323</td>\n",
       "      <td>-1.284527</td>\n",
       "      <td>-2.525800</td>\n",
       "      <td>1.195869</td>\n",
       "      <td>-1.066986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.272081</td>\n",
       "      <td>0.237548</td>\n",
       "      <td>-0.152154</td>\n",
       "      <td>0.654123</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>-0.054210</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.230681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>1.299207</td>\n",
       "      <td>0.608940</td>\n",
       "      <td>-0.429922</td>\n",
       "      <td>-2.432537</td>\n",
       "      <td>0.892208</td>\n",
       "      <td>1.645011</td>\n",
       "      <td>-0.327658</td>\n",
       "      <td>-2.380873</td>\n",
       "      <td>0.699112</td>\n",
       "      <td>-0.942508</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.272081</td>\n",
       "      <td>0.237548</td>\n",
       "      <td>-0.152154</td>\n",
       "      <td>0.654123</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>-0.054210</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.402754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>1.185846</td>\n",
       "      <td>-0.074296</td>\n",
       "      <td>0.019658</td>\n",
       "      <td>-1.958146</td>\n",
       "      <td>0.686428</td>\n",
       "      <td>0.947811</td>\n",
       "      <td>-0.239064</td>\n",
       "      <td>-1.941161</td>\n",
       "      <td>0.832770</td>\n",
       "      <td>-0.684379</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.272081</td>\n",
       "      <td>0.237548</td>\n",
       "      <td>-0.152154</td>\n",
       "      <td>0.654123</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>-0.054210</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.396090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.050730</td>\n",
       "      <td>0.245621</td>\n",
       "      <td>0.276231</td>\n",
       "      <td>-1.254308</td>\n",
       "      <td>0.216339</td>\n",
       "      <td>0.195184</td>\n",
       "      <td>0.310496</td>\n",
       "      <td>-1.122111</td>\n",
       "      <td>0.744570</td>\n",
       "      <td>-0.620754</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.272081</td>\n",
       "      <td>0.237548</td>\n",
       "      <td>-0.152154</td>\n",
       "      <td>0.654123</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>-0.054210</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.340468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.884583</td>\n",
       "      <td>0.276845</td>\n",
       "      <td>1.220682</td>\n",
       "      <td>-0.550658</td>\n",
       "      <td>-0.325163</td>\n",
       "      <td>-0.665076</td>\n",
       "      <td>0.646370</td>\n",
       "      <td>-0.380077</td>\n",
       "      <td>-0.098503</td>\n",
       "      <td>-0.441901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.272081</td>\n",
       "      <td>0.237548</td>\n",
       "      <td>-0.152154</td>\n",
       "      <td>0.654123</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>-0.054210</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.343122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>996 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y  g_pop_NUTS2  lag1_under15  lag1_25_44  lag1_45_64   lag1_65  \\\n",
       "1    1.174051    -0.548777     -0.093245    0.898419   -0.966361  0.099501   \n",
       "2    1.045550    -0.040484      0.471993    0.883495   -0.742478 -0.353670   \n",
       "3    1.019228     0.218589     -0.776667    1.659671   -0.242451  0.083935   \n",
       "4    1.059381    -0.545698      0.518554    1.131020   -1.306223 -0.382344   \n",
       "5    1.071665    -0.389150      0.245252    1.114368   -1.186103 -0.239795   \n",
       "..        ...          ...           ...         ...         ...       ...   \n",
       "992  1.346680     0.772758     -0.370570   -2.604024    1.429803  1.807323   \n",
       "993  1.299207     0.608940     -0.429922   -2.432537    0.892208  1.645011   \n",
       "994  1.185846    -0.074296      0.019658   -1.958146    0.686428  0.947811   \n",
       "995  1.050730     0.245621      0.276231   -1.254308    0.216339  0.195184   \n",
       "996  0.884583     0.276845      1.220682   -0.550658   -0.325163 -0.665076   \n",
       "\n",
       "     lag2_15_24  lag2_25_44  lag1_g_15_24  lag1_g_25_44  ...  ctry_prop_45_64  \\\n",
       "1      0.376766    0.244323     -0.831105      1.281140  ...        -1.015873   \n",
       "2      0.387420    0.148629     -1.324951      1.413474  ...        -1.015873   \n",
       "3     -0.334807    0.948677     -1.452726      1.508788  ...        -1.015873   \n",
       "4      0.702631    0.551440     -1.219143      1.203153  ...        -1.015873   \n",
       "5      0.709157    0.366296     -1.215400      1.476652  ...        -1.015873   \n",
       "..          ...         ...           ...           ...  ...              ...   \n",
       "992   -1.284527   -2.525800      1.195869     -1.066986  ...        -0.272081   \n",
       "993   -0.327658   -2.380873      0.699112     -0.942508  ...        -0.272081   \n",
       "994   -0.239064   -1.941161      0.832770     -0.684379  ...        -0.272081   \n",
       "995    0.310496   -1.122111      0.744570     -0.620754  ...        -0.272081   \n",
       "996    0.646370   -0.380077     -0.098503     -0.441901  ...        -0.272081   \n",
       "\n",
       "     ctry_prop_65  lag2_ctry_prop_15_24  lag1_ctry_pop_g_under15  \\\n",
       "1       -0.942237              0.614698                 0.922978   \n",
       "2       -0.942237              0.614698                 0.922978   \n",
       "3       -0.942237              0.614698                 0.922978   \n",
       "4       -0.942237              0.614698                 0.922978   \n",
       "5       -0.942237              0.614698                 0.922978   \n",
       "..            ...                   ...                      ...   \n",
       "992      0.237548             -0.152154                 0.654123   \n",
       "993      0.237548             -0.152154                 0.654123   \n",
       "994      0.237548             -0.152154                 0.654123   \n",
       "995      0.237548             -0.152154                 0.654123   \n",
       "996      0.237548             -0.152154                 0.654123   \n",
       "\n",
       "     lag1_ctry_pop_g_45_64  lag1_ctry_pop_g_65  cluster_1  cluster_2  \\\n",
       "1                -0.788814           -0.861139          0          0   \n",
       "2                -0.788814           -0.861139          1          0   \n",
       "3                -0.788814           -0.861139          0          1   \n",
       "4                -0.788814           -0.861139          0          0   \n",
       "5                -0.788814           -0.861139          1          0   \n",
       "..                     ...                 ...        ...        ...   \n",
       "992              -0.509907           -0.054210          0          0   \n",
       "993              -0.509907           -0.054210          0          0   \n",
       "994              -0.509907           -0.054210          0          0   \n",
       "995              -0.509907           -0.054210          1          0   \n",
       "996              -0.509907           -0.054210          0          1   \n",
       "\n",
       "     cluster_3  urb_frac  \n",
       "1            1 -0.279664  \n",
       "2            0 -0.390826  \n",
       "3            0  4.277729  \n",
       "4            1 -0.557908  \n",
       "5            0 -0.467357  \n",
       "..         ...       ...  \n",
       "992          1 -0.230681  \n",
       "993          1 -0.402754  \n",
       "994          1 -0.396090  \n",
       "995          0 -0.340468  \n",
       "996          0 -0.343122  \n",
       "\n",
       "[996 rows x 30 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = train_data[selected_columns]\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m[selected_columns]\n\u001b[1;32m      2\u001b[0m target_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m          \u001b[38;5;66;03m# Name of the target column\u001b[39;00m\n\u001b[1;32m      3\u001b[0m num_neurons \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m             \u001b[38;5;66;03m# Number of neurons in the hidden layer\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "training_data = train_data[selected_columns]\n",
    "target_column = 'y'          # Name of the target column\n",
    "num_neurons = 7             # Number of neurons in the hidden layer\n",
    "learning_rate = 0.3         # Learning rate for the optimizer\n",
    "epochs_num = 25            # Number of epochs\n",
    "activation_fun = 'elu'      # Activation function for the hidden layer\n",
    "seed_to_set = 123            # Random seed for reproducibility\n",
    "\n",
    "# Call the fit_mlp function\n",
    "model = fit_mlp(training_data, \n",
    "                target_column, \n",
    "                num_neurons, \n",
    "                learning_rate, \n",
    "                epochs_num, \n",
    "                activation_fun, \n",
    "                seed_to_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SIEVE_funcs_all\n",
    "from SIEVE_funcs_all import sieve_fit_brulee_mlp_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_mlp(training_data, target_column, num_neurons, learning_rate, epochs_num, activation_fun, seed_to_set):\n",
    "    \"\"\"\n",
    "    Fits an MLP model using TensorFlow.\n",
    "\n",
    "    Parameters:\n",
    "    - training_data: pandas DataFrame containing the training data.\n",
    "    - target_column: str, name of the target column in the DataFrame.\n",
    "    - num_neurons: int, number of neurons in the hidden layer.\n",
    "    - learning_rate: float, learning rate for the optimizer.\n",
    "    - epochs_num: int, number of epochs to train the model.\n",
    "    - activation_fun: str, activation function for the hidden layer.\n",
    "    - seed_to_set: int, random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A trained TensorFlow model.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    tf.random.set_seed(seed_to_set)\n",
    "\n",
    "    # Split features and target\n",
    "    X = training_data.drop(columns=[target_column])\n",
    "    y = training_data[target_column]\n",
    "\n",
    "    # Define the model\n",
    "    model = Sequential([\n",
    "        Dense(num_neurons, activation=activation_fun),\n",
    "        Dense(1)  # I am solving a regression model here with a continuous output\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='mse',\n",
    "                  metrics=['mae', 'mse'], \n",
    "                  )  # Track both MAE and MSE\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X, y, epochs=epochs_num, batch_size=12, verbose = 0)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sieve_fit_brulee_mlp_20_old(train, test, num_neurons, learning_rate, activation_fun, epochs_init, rounds, seed_basis):\n",
    "    MAEs = []\n",
    "    candidate = []\n",
    "    print('initial step, I will produce', epochs_init ** (epochs_init - 1), 'models, each for', epochs_init, 'epochs')\n",
    "    # The first round is not competitive. The others are.\n",
    "\n",
    "    MAEs.append([])\n",
    "    for x in range(1, epochs_init ** (epochs_init - 1)+1):\n",
    "     print(x)\n",
    "\n",
    "     np.random.seed(seed_basis * x)\n",
    "    \n",
    "        # Fit the model using fit_mlp\n",
    "     model = fit_mlp(training_data=train, \n",
    "                        target_column='y',  # Assuming 'y' is the target column\n",
    "                        num_neurons=num_neurons,\n",
    "                        learning_rate=learning_rate,\n",
    "                        epochs_num=epochs_init,\n",
    "                        activation_fun=activation_fun,\n",
    "                        seed_to_set=seed_basis * x)\n",
    "        \n",
    "        # Predict and compute MAE\n",
    "     pred_test = model.predict(test.drop(columns='y'), verbose = 0 )\n",
    "     pred_train = model.predict(train.drop(columns='y'), verbose = 0 )\n",
    "        \n",
    "     mae_test = mean_absolute_error(test['y'], pred_test)\n",
    "     mae_train = mean_absolute_error(train['y'], pred_train)\n",
    "        \n",
    "     MAE = mae_test + mae_train\n",
    "     res_list = [seed_basis * x, MAE, model]\n",
    "     MAEs[0].append(res_list)\n",
    "     print(MAE)\n",
    "    \n",
    "    print('end of the first step')\n",
    "\n",
    "    # Find the best candidate\n",
    "   # candidate.append(MAEs[0])\n",
    "    candidate.append(extract_lowest(MAEs[0], epochs_init ** (epochs_init - 2)))\n",
    "\n",
    "    for i in range(1, rounds):\n",
    "        if i == rounds - 1:\n",
    "            print(\"final step\")\n",
    "            MAEs.append([])\n",
    "            for x in [item[0] for item in candidate[i - 1]]:\n",
    "                np.random.seed(x)\n",
    "                \n",
    "                # Fit the model using fit_mlp\n",
    "                model = fit_mlp(training_data=train, \n",
    "                                target_column='y',\n",
    "                                num_neurons=num_neurons,\n",
    "                                learning_rate=learning_rate,\n",
    "                                epochs_num=epochs_init ** ((i + 1)),\n",
    "                                activation_fun=activation_fun,\n",
    "                                seed_to_set=x)\n",
    "                \n",
    "                # Predict and compute MAE\n",
    "                pred_test = model.predict(test.drop(columns='y'), verbose = 0 )\n",
    "                pred_train = model.predict(train.drop(columns='y'), verbose = 0 )\n",
    "                \n",
    "                mae_test = mean_absolute_error(test['y'], pred_test)\n",
    "                mae_train = mean_absolute_error(train['y'], pred_train)\n",
    "                \n",
    "                MAE = mae_test + mae_train\n",
    "                res_list = [x, MAE, model]\n",
    "                MAEs[i].append(res_list)\n",
    "        else:\n",
    "            print(i, \"-th selective step\")\n",
    "            MAEs.append([])\n",
    "            for x in [item[0] for item in candidate[i - 1]]: # looks into the previous step candidate object\n",
    "                np.random.seed(x)\n",
    "                \n",
    "                # Fit the model using fit_mlp\n",
    "                model = fit_mlp(training_data=train, \n",
    "                                target_column='y',\n",
    "                                num_neurons=num_neurons,\n",
    "                                learning_rate=learning_rate,\n",
    "                                epochs_num=epochs_init ** ((i + 1)),\n",
    "                                activation_fun=activation_fun,\n",
    "                                seed_to_set=x)\n",
    "                \n",
    "                # Predict and compute MAE\n",
    "                pred_test = model.predict(test.drop(columns='y'))\n",
    "                pred_train = model.predict(train.drop(columns='y'))\n",
    "                \n",
    "                mae_test = mean_absolute_error(test['y'], pred_test)\n",
    "                mae_train = mean_absolute_error(train['y'], pred_train)\n",
    "                \n",
    "                MAE = mae_test + mae_train\n",
    "                res_list = [x, MAE, model]\n",
    "                MAEs[i].append(res_list)\n",
    "        \n",
    "        # Find the best candidate for this round\n",
    "        candidate.append(extract_lowest(MAEs[i], epochs_init ** (epochs_init - (i + 2))))\n",
    "    \n",
    "        return candidate\n",
    "\n",
    "\n",
    "def extract_lowest(my_list, n):\n",
    "    if len(my_list) == 0:\n",
    "        raise ValueError(\"The input list is empty.\")\n",
    "    \n",
    "    # Extract the second elements from each pair in the list\n",
    "    second_elements = [x[1] for x in my_list]\n",
    "    \n",
    "    # Find the indices of the n lowest elements\n",
    "    lowest_indices = np.argsort(second_elements)[:n]\n",
    "    \n",
    "    # Extract the n elements from the original list\n",
    "    selected_elements = [my_list[i] for i in lowest_indices]\n",
    "    \n",
    "    return selected_elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_seed(x, train, test, num_neurons, learning_rate, epochs_init, i, activation_fun):\n",
    "    np.random.seed(x)\n",
    "    print(x)\n",
    "    # Fit the model using fit_mlp\n",
    "    model = fit_mlp(training_data=train, \n",
    "                    target_column='y',\n",
    "                    num_neurons=num_neurons,\n",
    "                    learning_rate=learning_rate,\n",
    "                    epochs_num=epochs_init ** (i + 1),\n",
    "                    activation_fun=activation_fun,\n",
    "                    seed_to_set=x)\n",
    "    \n",
    "    # Predict and compute MAE\n",
    "    pred_test = model.predict(test.drop(columns='y'), verbose=0)\n",
    "    pred_train = model.predict(train.drop(columns='y'), verbose=0)\n",
    "    \n",
    "    mae_test = mean_absolute_error(test['y'], pred_test)\n",
    "    mae_train = mean_absolute_error(train['y'], pred_train)\n",
    "    \n",
    "    MAE = mae_test + mae_train\n",
    "    return [x, MAE, model]\n",
    "\n",
    "\n",
    "from pathos.multiprocessing import ProcessingPool as ProcessPoolExecutor\n",
    "\n",
    "\n",
    "\n",
    "def sieve_fit_brulee_mlp_20(train, test, num_neurons, learning_rate, activation_fun, epochs_init, rounds, seed_basis):\n",
    "   MAEs = []\n",
    "   candidate = []\n",
    "   print('initial step, I will produce', epochs_init ** (epochs_init - 1), 'models, each for', epochs_init, 'epochs')\n",
    "\n",
    "   # The first round is not competitive. The others are.\n",
    "   MAEs.append([])\n",
    "   seeds = range(1*seed_basis, (epochs_init ** (epochs_init - 1))+seed_basis)\n",
    "\n",
    "   with ProcessPoolExecutor(nodes = 4) as executor:\n",
    "      results = executor.map(lambda x: process_seed(x, train, test, num_neurons, learning_rate, epochs_init, 0, activation_fun), seeds)\n",
    "      MAEs[0].extend(results)\n",
    "\n",
    "\n",
    "    # Find the best candidate\n",
    "    # candidate.append(MAEs[0])\n",
    "   print('I estimate', epochs_init ** (epochs_init - 1) ,' different models')\n",
    "\n",
    "   candidate.append(extract_lowest(MAEs[0], epochs_init ** (epochs_init - 2)))\n",
    "    \n",
    "   \n",
    "   for i in range(1, rounds):\n",
    "    if i == rounds - 1:\n",
    "      print(\"final step\")\n",
    "    else: \n",
    "      print(i, '-th competitive step')\n",
    "   MAEs.append([])\n",
    "    # Vectorized-like operation with parallel execution\n",
    "   seeds = [item[0] for item in candidate[i - 1]]\n",
    "   MAEs[i] = []\n",
    "   with ProcessPoolExecutor(nodes = 4) as executor:\n",
    "     results = executor.map(lambda x: process_seed(x, train, test, num_neurons, learning_rate, epochs_init, i, activation_fun), seeds)\n",
    "     MAEs[i].extend(results)\n",
    "   candidate.append(extract_lowest(MAEs[i], epochs_init ** (epochs_init - (i + 2))))\n",
    "   \n",
    "   return candidate\n",
    "\n",
    "\n",
    "# # Vectorized-like operation with parallel execution\n",
    "# seeds = [item[0] for item in candidate[i - 1]]\n",
    "# MAEs[i] = []\n",
    "\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     results = executor.map(lambda x: process_seed(x, train, test, num_neurons, learning_rate, epochs_init, i, activation_fun), seeds)\n",
    "#     MAEs[i].extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial step, I will produce 625 models, each for 5 epochs\n",
      "1\n",
      "0.1327232256633778\n",
      "2\n",
      "0.18010808257332045\n",
      "3\n",
      "0.09339200899846875\n",
      "4\n",
      "0.171787663550407\n",
      "5\n",
      "0.13493820648859345\n",
      "6\n",
      "0.09093830673835668\n",
      "7\n",
      "0.13356089745141198\n",
      "8\n",
      "0.0902633589682573\n",
      "9\n",
      "0.16104609247847412\n",
      "10\n",
      "0.15453905085727604\n",
      "11\n",
      "0.09522636260084467\n",
      "12\n",
      "0.11064380645192778\n",
      "13\n",
      "0.21243986054360714\n",
      "14\n",
      "0.09552736525111843\n",
      "15\n",
      "0.153104825186894\n",
      "16\n",
      "0.16760411908842276\n",
      "17\n",
      "0.11699632991269285\n",
      "18\n",
      "0.13594724869454156\n",
      "19\n",
      "0.1103822964830248\n",
      "20\n",
      "0.14673005618592536\n",
      "21\n",
      "0.0914785587387549\n",
      "22\n",
      "0.1302363687937712\n",
      "23\n",
      "0.14603781917057929\n",
      "24\n",
      "0.11229884073180774\n",
      "25\n",
      "0.12263038830989162\n",
      "26\n",
      "0.13315913744045638\n",
      "27\n",
      "0.15052140475990133\n",
      "28\n",
      "0.12594284845042147\n",
      "29\n",
      "0.1374770457012967\n",
      "30\n",
      "0.1423416550076381\n",
      "31\n",
      "0.16896363477412232\n",
      "32\n",
      "0.17654182665382212\n",
      "33\n",
      "0.11008136762678733\n",
      "34\n",
      "0.15062090644000725\n",
      "35\n",
      "0.10826012257835396\n",
      "36\n",
      "0.10262693357983803\n",
      "37\n",
      "0.10984661650691156\n",
      "38\n",
      "0.18273129405514965\n",
      "39\n",
      "0.14357197716209555\n",
      "40\n",
      "0.15265551296476135\n",
      "41\n",
      "0.14779664584904129\n",
      "42\n",
      "0.12115229881156153\n",
      "43\n",
      "0.13385935147518657\n",
      "44\n",
      "0.181044179979663\n",
      "45\n",
      "0.15904463035581862\n",
      "46\n",
      "0.1178966364273454\n",
      "47\n",
      "0.07872080222488903\n",
      "48\n",
      "0.09946167162736903\n",
      "49\n",
      "0.15630493619837463\n",
      "50\n",
      "0.16155626354704256\n",
      "51\n",
      "0.15598984166725544\n",
      "52\n",
      "0.14047215824156012\n",
      "53\n",
      "0.1423912238131482\n",
      "54\n",
      "0.21967909492196291\n",
      "55\n",
      "0.1463676332781395\n",
      "56\n",
      "0.2009280890295644\n",
      "57\n",
      "0.11253253176824711\n",
      "58\n",
      "0.1558470382990521\n",
      "59\n",
      "0.17558966891017622\n",
      "60\n",
      "0.13219717052623012\n",
      "61\n",
      "0.140293267353529\n",
      "62\n",
      "0.14075039151757301\n",
      "63\n",
      "0.20335928425187758\n",
      "64\n",
      "0.1273416437376257\n",
      "65\n",
      "0.08287385898146896\n",
      "66\n",
      "0.11317931039556663\n",
      "67\n",
      "0.12778159000997083\n",
      "68\n",
      "0.1848511226782333\n",
      "69\n",
      "0.09831461240768968\n",
      "70\n",
      "0.14793604672423882\n",
      "71\n",
      "0.12546202410209642\n",
      "72\n",
      "0.17585944170535564\n",
      "73\n",
      "0.11969108021948832\n",
      "74\n",
      "0.10718221746614742\n",
      "75\n",
      "0.18318649115797975\n",
      "76\n",
      "0.21930154507781188\n",
      "77\n",
      "0.11228162546477752\n",
      "78\n",
      "0.15604371687134685\n",
      "79\n",
      "0.08896664954318746\n",
      "80\n",
      "0.17263307011728835\n",
      "81\n",
      "0.1555940892566258\n",
      "82\n",
      "0.16759659940573918\n",
      "83\n",
      "0.14703305986622875\n",
      "84\n",
      "0.10852785656969405\n",
      "85\n",
      "0.15490474563525364\n",
      "86\n",
      "0.23940771725585863\n",
      "87\n",
      "0.13256873691579113\n",
      "88\n",
      "0.16523202910717538\n",
      "89\n",
      "0.11978872159303386\n",
      "90\n",
      "0.13893276280805608\n",
      "91\n",
      "0.14217660750688954\n",
      "92\n",
      "0.09245686487633142\n",
      "93\n",
      "0.09670610519529076\n",
      "94\n",
      "0.08789577972504148\n",
      "95\n",
      "0.12168756224049082\n",
      "96\n",
      "0.23057139115949982\n",
      "97\n",
      "0.1374310928504639\n",
      "98\n",
      "0.16895265611725552\n",
      "99\n",
      "0.14602245903470418\n",
      "100\n",
      "0.13015357609267944\n",
      "101\n",
      "0.09546555864165437\n",
      "102\n",
      "0.17436205853306616\n",
      "103\n",
      "0.10762925166507203\n",
      "104\n",
      "0.14968675823380495\n",
      "105\n",
      "0.15101561219590293\n",
      "106\n",
      "0.13940079551264672\n",
      "107\n",
      "0.1658685769424878\n",
      "108\n",
      "0.04640052642155757\n",
      "109\n",
      "0.09022309335468512\n",
      "110\n",
      "0.1103723851678117\n",
      "111\n",
      "0.13252494532404463\n",
      "112\n",
      "0.11103613141344061\n",
      "113\n",
      "0.14688598200524658\n",
      "114\n",
      "0.20780668602015762\n",
      "115\n",
      "0.1493224626298802\n",
      "116\n",
      "0.09968154414296596\n",
      "117\n",
      "0.1582080171160175\n",
      "118\n",
      "0.11044156569970345\n",
      "119\n",
      "0.20945928564990596\n",
      "120\n",
      "0.168837128060784\n",
      "121\n",
      "0.14695209864962622\n",
      "122\n",
      "0.1509862039514816\n",
      "123\n",
      "0.12728334079391598\n",
      "124\n",
      "0.17628003050813418\n",
      "125\n",
      "0.1005761544812109\n",
      "126\n",
      "0.14884058742174486\n",
      "127\n",
      "0.13909141117341095\n",
      "128\n",
      "0.11029908540658678\n",
      "129\n",
      "0.13555838683577204\n",
      "130\n",
      "0.07521700017886525\n",
      "131\n",
      "0.14358469111783362\n",
      "132\n",
      "0.1405943324619845\n",
      "133\n",
      "0.19858611732747344\n",
      "134\n",
      "0.10766135195934048\n",
      "135\n",
      "0.11016211017067087\n",
      "136\n",
      "0.08712055516361772\n",
      "137\n",
      "0.1641287346726969\n",
      "138\n",
      "0.1527111573151419\n",
      "139\n",
      "0.16276009239787276\n",
      "140\n",
      "0.07651964907039763\n",
      "141\n",
      "0.13643588661301492\n",
      "142\n",
      "0.12645140008856312\n",
      "143\n",
      "0.12190038790341393\n",
      "144\n",
      "0.07422150631450739\n",
      "145\n",
      "0.10694945177781036\n",
      "146\n",
      "0.09533100595918634\n",
      "147\n",
      "0.14231605455949048\n",
      "148\n",
      "0.13888337439815654\n",
      "149\n",
      "0.11516198381128731\n",
      "150\n",
      "0.1399436164724892\n",
      "151\n",
      "0.09463382786112531\n",
      "152\n",
      "0.16682706600011946\n",
      "153\n",
      "0.07550307072202717\n",
      "154\n",
      "0.09180355205973013\n",
      "155\n",
      "0.1066203703824935\n",
      "156\n",
      "0.13559661676222454\n",
      "157\n",
      "0.11928359826315227\n",
      "158\n",
      "0.11731431175338758\n",
      "159\n",
      "0.1700884501800245\n",
      "160\n",
      "0.15850960365222677\n",
      "161\n",
      "0.1296085437109495\n",
      "162\n",
      "0.14974776496335881\n",
      "163\n",
      "0.12359701422064034\n",
      "164\n",
      "0.12333384763197242\n",
      "165\n",
      "0.138978517991803\n",
      "166\n",
      "0.12172279894971967\n",
      "167\n",
      "0.07622566330558982\n",
      "168\n",
      "0.18910229046219426\n",
      "169\n",
      "0.1798662549953282\n",
      "170\n",
      "0.1368499079305703\n",
      "171\n",
      "0.14778529615854757\n",
      "172\n",
      "0.16763808945636854\n",
      "173\n",
      "0.10197664919548863\n",
      "174\n",
      "0.1359035906911218\n",
      "175\n",
      "0.09729154880725374\n",
      "176\n",
      "0.08766286944476515\n",
      "177\n",
      "0.13870672344574606\n",
      "178\n",
      "0.17308543458552952\n",
      "179\n",
      "0.12246725078492626\n",
      "180\n",
      "0.1412320126658818\n",
      "181\n",
      "0.06954144489327108\n",
      "182\n",
      "0.10791918364904739\n",
      "183\n",
      "0.14631308828571715\n",
      "184\n",
      "0.14485617812732465\n",
      "185\n",
      "0.0783490405804819\n",
      "186\n",
      "0.24107260908106792\n",
      "187\n",
      "0.14219498233923977\n",
      "188\n",
      "0.1683268585276178\n",
      "189\n",
      "0.144938282620037\n",
      "190\n",
      "0.14230191807347464\n",
      "191\n",
      "0.15001413353762072\n",
      "192\n",
      "0.11935628432384296\n",
      "193\n",
      "0.12869344810696148\n",
      "194\n",
      "0.1507873160046911\n",
      "195\n",
      "0.10392713555882055\n",
      "196\n",
      "0.09641737002062784\n",
      "197\n",
      "0.14037711887203558\n",
      "198\n",
      "0.12317366912075324\n",
      "199\n",
      "0.12033886851590017\n",
      "200\n",
      "0.1442810820861219\n",
      "201\n",
      "0.13311974673510046\n",
      "202\n",
      "0.13427792897511184\n",
      "203\n",
      "0.09751696954023985\n",
      "204\n",
      "0.07359203051537418\n",
      "205\n",
      "0.12810874105582148\n",
      "206\n",
      "0.1553240282746992\n",
      "207\n",
      "0.13148952550417933\n",
      "208\n",
      "0.1580766473962655\n",
      "209\n",
      "0.14107632500058026\n",
      "210\n",
      "0.15505405476580247\n",
      "211\n",
      "0.1561644282997769\n",
      "212\n",
      "0.13547496262214903\n",
      "213\n",
      "0.21064712468681474\n",
      "214\n",
      "0.12651285082272531\n",
      "215\n",
      "0.11870414853680494\n",
      "216\n",
      "0.15167620058585107\n",
      "217\n",
      "0.12366875082774885\n",
      "218\n",
      "0.11128197309390772\n",
      "219\n",
      "0.13017197491840915\n",
      "220\n",
      "0.2073177058795312\n",
      "221\n",
      "0.15687308626611718\n",
      "222\n",
      "0.08018384949336269\n",
      "223\n",
      "0.08598711232345134\n",
      "224\n",
      "0.1513210332656429\n",
      "225\n",
      "0.09382759811273783\n",
      "226\n",
      "0.16346644353471865\n",
      "227\n",
      "0.13152869409534482\n",
      "228\n",
      "0.07980669080085984\n",
      "229\n",
      "0.10204579516354284\n",
      "230\n",
      "0.1060677332490527\n",
      "231\n",
      "0.13188150743221672\n",
      "232\n",
      "0.11467514701186042\n",
      "233\n",
      "0.1337718452387999\n",
      "234\n",
      "0.13604074126450472\n",
      "235\n",
      "0.13263873681759336\n",
      "236\n",
      "0.14549523976800788\n",
      "237\n",
      "0.1368201081180772\n",
      "238\n",
      "0.12364349775023939\n",
      "239\n",
      "0.11746123195856036\n",
      "240\n",
      "0.11696330138586922\n",
      "241\n",
      "0.14670923240612\n",
      "242\n",
      "0.1313391279637156\n",
      "243\n",
      "0.13610563899247183\n",
      "244\n",
      "0.15500466111330372\n",
      "245\n",
      "0.13454416537762912\n",
      "246\n",
      "0.12243820016199092\n",
      "247\n",
      "0.08642998102772276\n",
      "248\n",
      "0.177934049307971\n",
      "249\n",
      "0.19804298150197666\n",
      "250\n",
      "0.21474898056004976\n",
      "251\n",
      "0.12131261680477123\n",
      "252\n",
      "0.051571786901840305\n",
      "253\n",
      "0.15694828341495629\n",
      "254\n",
      "0.09470047785719951\n",
      "255\n",
      "0.19625160011846216\n",
      "256\n",
      "0.11680537714356792\n",
      "257\n",
      "0.15452571797896658\n",
      "258\n",
      "0.14709316566673547\n",
      "259\n",
      "0.12131111412995081\n",
      "260\n",
      "0.11325398061642983\n",
      "261\n",
      "0.10294381297730207\n",
      "262\n",
      "0.14291126648651192\n",
      "263\n",
      "0.19799972506593067\n",
      "264\n",
      "0.132110739861453\n",
      "265\n",
      "0.15279628987894311\n",
      "266\n",
      "0.17244491553948718\n",
      "267\n",
      "0.11793029297126568\n",
      "268\n",
      "0.10179194478363535\n",
      "269\n",
      "0.159961603692654\n",
      "270\n",
      "0.09625245436787305\n",
      "271\n",
      "0.15284217973535563\n",
      "272\n",
      "0.08086989616203033\n",
      "273\n",
      "0.08227525080959236\n",
      "274\n",
      "0.15041553828379708\n",
      "275\n",
      "0.12155271717668631\n",
      "276\n",
      "0.11195567570773972\n",
      "277\n",
      "0.17453765798224757\n",
      "278\n",
      "0.1837567733314523\n",
      "279\n",
      "0.16504415944913586\n",
      "280\n",
      "0.10157511005082234\n",
      "281\n",
      "0.10041151596206685\n",
      "282\n",
      "0.14451302308060854\n",
      "283\n",
      "0.14678146709802337\n",
      "284\n",
      "0.1206471235348881\n",
      "285\n",
      "0.15625058521644541\n",
      "286\n",
      "0.10343870473484723\n",
      "287\n",
      "0.11099350735009939\n",
      "288\n",
      "0.14883462930991745\n",
      "289\n",
      "0.14569756420519098\n",
      "290\n",
      "0.19246932767059435\n",
      "291\n",
      "0.13553355332417552\n",
      "292\n",
      "0.12858993997937268\n",
      "293\n",
      "0.14214393089023408\n",
      "294\n",
      "0.0782566516997966\n",
      "295\n",
      "0.08218936822272851\n",
      "296\n",
      "0.1168367111752972\n",
      "297\n",
      "0.15538962716680593\n",
      "298\n",
      "0.08496867872138396\n",
      "299\n",
      "0.21186418003566054\n",
      "300\n",
      "0.1780340365457312\n",
      "301\n",
      "0.08891578608711193\n",
      "302\n",
      "0.11277821673183605\n",
      "303\n",
      "0.08103946164245329\n",
      "304\n",
      "0.12699290163049742\n",
      "305\n",
      "0.20686688979023538\n",
      "306\n",
      "0.1489328959824263\n",
      "307\n",
      "0.18263198532539915\n",
      "308\n",
      "0.17277796425655476\n",
      "309\n",
      "0.17476273262199313\n",
      "310\n",
      "0.19257642207915643\n",
      "311\n",
      "0.16514894784474587\n",
      "312\n",
      "0.13023671563580735\n",
      "313\n",
      "0.09251438787407607\n",
      "314\n",
      "0.09108701973941034\n",
      "315\n",
      "0.12463554253247729\n",
      "316\n",
      "0.15842609274101443\n",
      "317\n",
      "0.1496689551279832\n",
      "318\n",
      "0.10236600798656795\n",
      "319\n",
      "0.1584431791753516\n",
      "320\n",
      "0.12942120180751124\n",
      "321\n",
      "0.10989120915291717\n",
      "322\n",
      "0.195647216883894\n",
      "323\n",
      "0.0775548733869974\n",
      "324\n",
      "0.06421584466390481\n",
      "325\n",
      "0.14611716326806617\n",
      "326\n",
      "0.11876510214212002\n",
      "327\n",
      "0.17214289144335632\n",
      "328\n",
      "0.12888500191917168\n",
      "329\n",
      "0.13529761517678618\n",
      "330\n",
      "0.13366666665566124\n",
      "331\n",
      "0.0684218691980073\n",
      "332\n",
      "0.12817174262758257\n",
      "333\n",
      "0.14015876677129918\n",
      "334\n",
      "0.0940847425990444\n",
      "335\n",
      "0.08878917789165003\n",
      "336\n",
      "0.15460236611401063\n",
      "337\n",
      "0.16880881998880437\n",
      "338\n",
      "0.19200599152508588\n",
      "339\n",
      "0.11265944910531667\n",
      "340\n",
      "0.1289038222486512\n",
      "341\n",
      "0.11683691835917659\n",
      "342\n",
      "0.1624273303774928\n",
      "343\n",
      "0.17453438398861926\n",
      "344\n",
      "0.12856755693759236\n",
      "345\n",
      "0.13824068389037725\n",
      "346\n",
      "0.1891536133131928\n",
      "347\n",
      "0.12653248904398431\n",
      "348\n",
      "0.18131076607407992\n",
      "349\n",
      "0.12281587908784788\n",
      "350\n",
      "0.16886991648923408\n",
      "351\n",
      "0.10330446763579829\n",
      "352\n",
      "0.1620391213872901\n",
      "353\n",
      "0.10052747864133135\n",
      "354\n",
      "0.11919306532441341\n",
      "355\n",
      "0.18379375364144118\n",
      "356\n",
      "0.14145357202659048\n",
      "357\n",
      "0.14882245988956047\n",
      "358\n",
      "0.10051480990415834\n",
      "359\n",
      "0.16966785197438572\n",
      "360\n",
      "0.11056927793975976\n",
      "361\n",
      "0.14784108516166522\n",
      "362\n",
      "0.12665452775688046\n",
      "363\n",
      "0.1776832015889329\n",
      "364\n",
      "0.12922429240943056\n",
      "365\n",
      "0.12241554963015261\n",
      "366\n",
      "0.14068539563796523\n",
      "367\n",
      "0.18786253949984216\n",
      "368\n",
      "0.17039723270828913\n",
      "369\n",
      "0.25168566279345966\n",
      "370\n",
      "0.1153221697165807\n",
      "371\n",
      "0.10340759298351263\n",
      "372\n",
      "0.15720664009505203\n",
      "373\n",
      "0.10225659908810483\n",
      "374\n",
      "0.1435756838856052\n",
      "375\n",
      "0.1188857544801461\n",
      "376\n",
      "0.1378108388301506\n",
      "377\n",
      "0.16567684377364095\n",
      "378\n",
      "0.12404645814967978\n",
      "379\n",
      "0.058075334772268075\n",
      "380\n",
      "0.11966399307332905\n",
      "381\n",
      "0.1724795339621555\n",
      "382\n",
      "0.19781095733833065\n",
      "383\n",
      "0.10961556427527422\n",
      "384\n",
      "0.22122670381713716\n",
      "385\n",
      "0.17178384809738945\n",
      "386\n",
      "0.1477581320478078\n",
      "387\n",
      "0.1504586106850788\n",
      "388\n",
      "0.091595898930625\n",
      "389\n",
      "0.13521256198587975\n",
      "390\n",
      "0.090283394684605\n",
      "391\n",
      "0.11488093770202107\n",
      "392\n",
      "0.1965178829995754\n",
      "393\n",
      "0.14802710581025974\n",
      "394\n",
      "0.13850133694589917\n",
      "395\n",
      "0.17653691904672486\n",
      "396\n",
      "0.1366092431644722\n",
      "397\n",
      "0.09988880214557061\n",
      "398\n",
      "0.18516693784568788\n",
      "399\n",
      "0.16811911219966102\n",
      "400\n",
      "0.15788714719282296\n",
      "401\n",
      "0.153709777154134\n",
      "402\n",
      "0.13912760953751765\n",
      "403\n",
      "0.1375001503086032\n",
      "404\n",
      "0.09148659251152821\n",
      "405\n",
      "0.0983997680572927\n",
      "406\n",
      "0.17760141638638094\n",
      "407\n",
      "0.10397959120505881\n",
      "408\n",
      "0.1625249141285311\n",
      "409\n",
      "0.11405884612582959\n",
      "410\n",
      "0.18741597108079977\n",
      "411\n",
      "0.12171923646741392\n",
      "412\n",
      "0.16167963964117446\n",
      "413\n",
      "0.16864705403485294\n",
      "414\n",
      "0.12809193431236574\n",
      "415\n",
      "0.12196907134004878\n",
      "416\n",
      "0.15135727823108908\n",
      "417\n",
      "0.16850854492703737\n",
      "418\n",
      "0.1732631988045862\n",
      "419\n",
      "0.1640118934651409\n",
      "420\n",
      "0.17355330449974882\n",
      "421\n",
      "0.12567990433625983\n",
      "422\n",
      "0.1807390179328317\n",
      "423\n",
      "0.16478324857128468\n",
      "424\n",
      "0.13301722200137517\n",
      "425\n",
      "0.12015008561877155\n",
      "426\n",
      "0.11209831849626131\n",
      "427\n",
      "0.12088980225732665\n",
      "428\n",
      "0.13151429376664708\n",
      "429\n",
      "0.12255729867160436\n",
      "430\n",
      "0.23971483253091785\n",
      "431\n",
      "0.15750136269329143\n",
      "432\n",
      "0.1549408166635601\n",
      "433\n",
      "0.16284882470655676\n",
      "434\n",
      "0.11745853241839889\n",
      "435\n",
      "0.2013175378321822\n",
      "436\n",
      "0.1928532227305498\n",
      "437\n",
      "0.12090977896896722\n",
      "438\n",
      "0.12781082292800075\n",
      "439\n",
      "0.19536698503812153\n",
      "440\n",
      "0.11590685701309272\n",
      "441\n",
      "0.09014083381616456\n",
      "442\n",
      "0.07517126722522632\n",
      "443\n",
      "0.16239190903405396\n",
      "444\n",
      "0.1090564253152197\n",
      "445\n",
      "0.10649780432423227\n",
      "446\n",
      "0.16842384695927726\n",
      "447\n",
      "0.12088918830762212\n",
      "448\n",
      "0.1718124186266935\n",
      "449\n",
      "0.15679802076355492\n",
      "450\n",
      "0.15625005672273945\n",
      "451\n",
      "0.17606894655897504\n",
      "452\n",
      "0.09425600083186236\n",
      "453\n",
      "0.1425220215059378\n",
      "454\n",
      "0.09412368919769348\n",
      "455\n",
      "0.22038425029950182\n",
      "456\n",
      "0.14046048210950834\n",
      "457\n",
      "0.12993501333502028\n",
      "458\n",
      "0.23382641398143217\n",
      "459\n",
      "0.17832345406101552\n",
      "460\n",
      "0.13447413543377967\n",
      "461\n",
      "0.12369808921763878\n",
      "462\n",
      "0.13213631022115463\n",
      "463\n",
      "0.13703339505282786\n",
      "464\n",
      "0.1488745094961158\n",
      "465\n",
      "0.13930334940172662\n",
      "466\n",
      "0.149267011335285\n",
      "467\n",
      "0.12551162484466036\n",
      "468\n",
      "0.07677379326318236\n",
      "469\n",
      "0.12027756006733717\n",
      "470\n",
      "0.18166187953184382\n",
      "471\n",
      "0.1310965400562163\n",
      "472\n",
      "0.14515498538402727\n",
      "473\n",
      "0.11449113975773896\n",
      "474\n",
      "0.14564561156656203\n",
      "475\n",
      "0.12281690367525702\n",
      "476\n",
      "0.12683835496346071\n",
      "477\n",
      "0.14504475492042074\n",
      "478\n",
      "0.13860120342912632\n",
      "479\n",
      "0.1336293264069372\n",
      "480\n",
      "0.1672117872414588\n",
      "481\n",
      "0.13951486141142333\n",
      "482\n",
      "0.20083557140355945\n",
      "483\n",
      "0.09102489340935424\n",
      "484\n",
      "0.10152431687085064\n",
      "485\n",
      "0.14269338535830192\n",
      "486\n",
      "0.11771294844853673\n",
      "487\n",
      "0.11911552207197937\n",
      "488\n",
      "0.12790390027931098\n",
      "489\n",
      "0.1336635658613734\n",
      "490\n",
      "0.10850179348069557\n",
      "491\n",
      "0.1319941902495594\n",
      "492\n",
      "0.1403016101245076\n",
      "493\n",
      "0.10079463880989772\n",
      "494\n",
      "0.11750032728162234\n",
      "495\n",
      "0.16255957015267497\n",
      "496\n",
      "0.10746978021335106\n",
      "497\n",
      "0.1332145883894456\n",
      "498\n",
      "0.12560458072910735\n",
      "499\n",
      "0.15298008680023173\n",
      "500\n",
      "0.11531047877012807\n",
      "501\n",
      "0.1319231421468891\n",
      "502\n",
      "0.15106933300856135\n",
      "503\n",
      "0.10055423378191639\n",
      "504\n",
      "0.11868086389145463\n",
      "505\n",
      "0.11854938470721677\n",
      "506\n",
      "0.17052083950797256\n",
      "507\n",
      "0.14114353231973126\n",
      "508\n",
      "0.12348625871006452\n",
      "509\n",
      "0.14518471671080618\n",
      "510\n",
      "0.11537249648101322\n",
      "511\n",
      "0.11433186258170921\n",
      "512\n",
      "0.1448420155647639\n",
      "513\n",
      "0.12854726992636062\n",
      "514\n",
      "0.15275248643833383\n",
      "515\n",
      "0.13118251783216944\n",
      "516\n",
      "0.12734037433249623\n",
      "517\n",
      "0.12149734562585293\n",
      "518\n",
      "0.1433298785205731\n",
      "519\n",
      "0.13623052489122237\n",
      "520\n",
      "0.14710855726426258\n",
      "521\n",
      "0.15555088590876437\n",
      "522\n",
      "0.15328873272341367\n",
      "523\n",
      "0.13789673525939045\n",
      "524\n",
      "0.15535340099874373\n",
      "525\n",
      "0.16481234028813108\n",
      "526\n",
      "0.16201683829835162\n",
      "527\n",
      "0.1053918615306605\n",
      "528\n",
      "0.2440941139621402\n",
      "529\n",
      "0.1739066840634576\n",
      "530\n",
      "0.11510586470177567\n",
      "531\n",
      "0.13976039760124265\n",
      "532\n",
      "0.1424700247024175\n",
      "533\n",
      "0.14973145616626543\n",
      "534\n",
      "0.1602807203433646\n",
      "535\n",
      "0.15305042152122444\n",
      "536\n",
      "0.15233222142999162\n",
      "537\n",
      "0.14097980748513095\n",
      "538\n",
      "0.15008055793201214\n",
      "539\n",
      "0.1494507980871663\n",
      "540\n",
      "0.1911830078507585\n",
      "541\n",
      "0.1130974370668435\n",
      "542\n",
      "0.14405594705611885\n",
      "543\n",
      "0.16531061289079993\n",
      "544\n",
      "0.10638795514827155\n",
      "545\n",
      "0.16748755186565945\n",
      "546\n",
      "0.13486793274742143\n",
      "547\n",
      "0.115255306714623\n",
      "548\n",
      "0.13043916445229542\n",
      "549\n",
      "0.16016545675543914\n",
      "550\n",
      "0.20242032868074195\n",
      "551\n",
      "0.13606001972548298\n",
      "552\n",
      "0.1194229528715919\n",
      "553\n",
      "0.123409926051861\n",
      "554\n",
      "0.10001042246666869\n",
      "555\n",
      "0.16616738882636348\n",
      "556\n",
      "0.18513602904171214\n",
      "557\n",
      "0.16304818817779987\n",
      "558\n",
      "0.0790041242732778\n",
      "559\n",
      "0.10323275817355564\n",
      "560\n",
      "0.12336901297375312\n",
      "561\n",
      "0.18125684009123266\n",
      "562\n",
      "0.13733325100231952\n",
      "563\n",
      "0.11145022748181804\n",
      "564\n",
      "0.10603956476783766\n",
      "565\n",
      "0.15491210562434243\n",
      "566\n",
      "0.14680181547988544\n",
      "567\n",
      "0.11858904529205072\n",
      "568\n",
      "0.1734434641604709\n",
      "569\n",
      "0.22957811949849444\n",
      "570\n",
      "0.10248996292709944\n",
      "571\n",
      "0.12221256137045347\n",
      "572\n",
      "0.0947746293647943\n",
      "573\n",
      "0.09604468099423491\n",
      "574\n",
      "0.1488368993379689\n",
      "575\n",
      "0.10643530171737553\n",
      "576\n",
      "0.16109224157531804\n",
      "577\n",
      "0.1371451058657543\n",
      "578\n",
      "0.16870468136231395\n",
      "579\n",
      "0.12668193345007553\n",
      "580\n",
      "0.1145552553399484\n",
      "581\n",
      "0.1632394589205906\n",
      "582\n",
      "0.13706240515340218\n",
      "583\n",
      "0.12855128889970988\n",
      "584\n",
      "0.15624298365987438\n",
      "585\n",
      "0.1521940503355195\n",
      "586\n",
      "0.14719036213536657\n",
      "587\n",
      "0.13226179137275007\n",
      "588\n",
      "0.13644997844268833\n",
      "589\n",
      "0.14240824714631423\n",
      "590\n",
      "0.10817291728345976\n",
      "591\n",
      "0.0744476287269865\n",
      "592\n",
      "0.1601702325282221\n",
      "593\n",
      "0.12791207437384106\n",
      "594\n",
      "0.13395728505354715\n",
      "595\n",
      "0.1992112467964492\n",
      "596\n",
      "0.14415561464707533\n",
      "597\n",
      "0.14332149233707103\n",
      "598\n",
      "0.14186198899791974\n",
      "599\n",
      "0.17473810866037232\n",
      "600\n",
      "0.153748670070881\n",
      "601\n",
      "0.1037637805123828\n",
      "602\n",
      "0.11453651919929725\n",
      "603\n",
      "0.14935594827041598\n",
      "604\n",
      "0.1411757288100985\n",
      "605\n",
      "0.1297039953076252\n",
      "606\n",
      "0.1438045829248235\n",
      "607\n",
      "0.14675150533459863\n",
      "608\n",
      "0.15026642182244004\n",
      "609\n",
      "0.10373116483499839\n",
      "610\n",
      "0.08741228213318433\n",
      "611\n",
      "0.1433013678309792\n",
      "612\n",
      "0.16371187018007266\n",
      "613\n",
      "0.16497944787569657\n",
      "614\n",
      "0.19635254050698775\n",
      "615\n",
      "0.16552255500294008\n",
      "616\n",
      "0.13649702472575606\n",
      "617\n",
      "0.11912992643894327\n",
      "618\n",
      "0.07428116775640257\n",
      "619\n",
      "0.14519814068429898\n",
      "620\n",
      "0.08544748666343895\n",
      "621\n",
      "0.11595620760347423\n",
      "622\n",
      "0.1513961162905465\n",
      "623\n",
      "0.10180218664913426\n",
      "624\n",
      "0.11529479201575099\n",
      "625\n",
      "0.13889251009066994\n",
      "end of the first step\n",
      "final step\n"
     ]
    }
   ],
   "source": [
    "train = train_data[selected_columns]\n",
    "test = test_data[selected_columns]\n",
    "formula_mod  = \"y ~ g_pop_NUTS2 + lag1_under15 + lag1_25_44 + lag1_45_64 + lag1_65 + lag2_15_24 + lag2_25_44 + lag1_g_15_24 + lag1_g_25_44 + lag1_g_45_64 + lag1_g_65 + log_quot + lag1_quot_15_24 + lag1_quot_25_44 + lag1_quot_45_64 + lag1_quot_65 + lag2_quot_15_24 + ctry_prop_under15 + ctry_prop_15_24 + ctry_prop_45_64 + ctry_prop_65 + lag2_ctry_prop_15_24 + lag1_ctry_pop_g_under15 + lag1_ctry_pop_g_45_64 + lag1_ctry_pop_g_65 + cluster_1 + cluster_2 + cluster_3 + urb_frac\"\n",
    "\n",
    "target_column = 'y'          # Name of the target column\n",
    "\n",
    "num_neurons = 7             # Number of neurons in the hidden layer\n",
    "learning_rate = 0.3         # Learning rate for the optimizer\n",
    "activation_fun = 'elu'      # Activation function for the hidden layer\n",
    "seed_basis = 100        # Random seed for reproducibility\n",
    "rounds = 2\n",
    "epochs_init = 5\n",
    "\n",
    "\n",
    "SIEVE_test_1 = sieve_fit_brulee_mlp_20_old(train, test, num_neurons, learning_rate, activation_fun, epochs_init, rounds, seed_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SIEVE_test_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sublist_lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(sublist) \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m \u001b[43mSIEVE_test_1\u001b[49m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(sublist_lengths)  \u001b[38;5;66;03m# Output: [3, 2, 4]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SIEVE_test_1' is not defined"
     ]
    }
   ],
   "source": [
    "sublist_lengths = [len(sublist) for sublist in SIEVE_test_1] # oay attention, list and sublist and so on are names in pythin which are fixed and can be used for exampel in this case to see how many elements each of the sublists has\n",
    "print(sublist_lengths)  # Output: [3, 2, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "second_elements = [item[1] for item in SIEVE_test_1[0]]\n",
    "second_elements = [0,1,5,6,7,21,4,5,5,7,8,89]\n",
    "lowest_indices = np.argsort(second_elements)[:2]\n",
    "lowest_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08659547909682219, 0.10670018584518341, 0.11130967326739472]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_elements\n",
    "    # Find the indices of the n lowest elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, rounds+1):   \n",
    "   print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am the new function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAEs = []\n",
    "candidate = []\n",
    "print('I am the new function')\n",
    "\n",
    "    # The first round is not competitive. The others are.\n",
    "MAEs.append([])\n",
    "MAEs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
